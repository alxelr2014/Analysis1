\chapter{Multivariable Calculus}
\thispagestyle{headings}
\section{Linear Algebra}
\subsection{Vector Spaces}

\begin{definition}[Normed vector space]
    Let \(V\) be a vector space. A \textbf{norm} is a real valued function \(\norm: V \to \Reals\) which has the following properties
    \begin{enumerate}
        \item \(\forall x \in V, \; \norm[x] > 0\).
        \item \(\norm[x] = 0 \implies x = 0\).
        \item \(\forall x \in V \; \forall \alpha \in \Field, \; \norm[\alpha x] = \abs[\alpha] \norm[x]\).
        \item \(\forall x,y \in V \; \norm[x+y] \leq \norm[x] + \norm[y]\).
    \end{enumerate}
\end{definition}

Each normed vector space induces a metric space \(\metricSpace{V}{d}\) where \(\func{d}{x,y} = \norm[y - x]\).

\begin{theorem}
    In every normed space \(\normedSpace{V}{\norm}\) we have
    \begin{equation*}
        \abs[ {\norm[v] - \norm[w]}] \leq \norm[v - w]
    \end{equation*}
    Hence the norm is Lipschitz continuous.
\end{theorem}


\begin{definition}
    Assume \(V\) is a vector space and let \(\norm_1, \; \norm_2\) be two norms for \(V\). They are said to be equivalent when
    \begin{equation*}
        \exists c_1,c_2 > 0 \; \forall x : \qquad c_1 \norm[x]_1 \leq \norm[x]_2 \leq c_2 \norm[x]
    \end{equation*}
\end{definition}

To check if the above definition is indeed an equivalence relation, we must show that following:
\begin{description}
    \item [Reflexive] \(\norm_1 \sim \norm_1\).
    \item [Symmetric] \(\norm_1 \sim \norm_2 \implies \norm_2 \sim \norm_1\).
    \item [Transitive] \( \norm_1 \sim \norm_2 , \; \norm_2 \sim \norm_3 \implies \norm_1 \sim \norm_3\).
\end{description}

\begin{remark}
    Equivalent norms induce equivalent metrics, hence they induce the same topology.
\end{remark}

\begin{theorem} \label{th:normsEquivalent}
    All norms defined on a finite dimensional vector space \(V\) are equivalent.
\end{theorem}

\begin{proof}
    Let \(\norm\) be an arbitrary norm on \(V\) and \(\{e_1, e_2, \dots , e_n\} \) be a basis of \(V\). Let \(\norm_2\) be \(L_2\)-norm (Euclidean norm). It will suffice to show \(\norm \sim \norm_2\). Let
    \begin{equation*}
        M = \max \! \left( \norm[e_1], \dots , \norm[e_n] \right)
    \end{equation*}
    Take \(x \in V\), writing \(x = \sum_{i = 1}^n {\xi_i e_i}\) we have:
    \begin{equation*}
        \norm[x] = \norm[\sum_{i = i}^n {\xi_i e_i}] \leq \sum_{i = 1}^n \abs[\xi_i] \norm[e_i] \leq M \sqrt{n} \norm[x]_2
    \end{equation*}
    Taking \(c_2 = M \sqrt{n}\) proves the right inequality. For the left inequality we need the following lemma
    \begin{lemma} \label{lm:ContinuityOfNorm}
        If \(V\) is a normed vector space with \(\norm_2\), as defined above, is viewed as metric space \(\metricSpace{V}{\norm_2}\) then \(\norm : V \to \Reals\) is continuous.
    \end{lemma}

    \begin{prooflemma}
        Let \(x_0 \in V\) and \(M\) be defined as above. For any \(\epsilon > 0\) consider \(\delta = \frac{\epsilon}{M \sqrt{n}}\) then if \(\norm[x - x_0]_2 < \delta\)
        \begin{equation*}
            \abs[{\norm[x] - \norm[x_0]}] \leq \norm[x - x_0] \leq M \sqrt{n} \norm[x - x_0] \leq \epsilon
        \end{equation*}
    \end{prooflemma}

    Now consider the sphere of radius \(r = 1\) centered at \(0\), \(\func{S_1}{0} = S_1 = \{x \in V : \norm[x]_2 = 1\}\). One can show that \(S\) is compact. Therefore, \(\norm[x]\) assumes its minimum on \(S\). Let \( a = \norm[x_0]\) be the minimum. Since \(0 \notin S\) then \(a > 0\). By letting \(y = x / \norm[x]_2 \), we have \(y \in S\) and thus \(a \leq \norm[y]\) which is
    \begin{equation*}
        a \norm[x]_2 \leq \norm[x]
    \end{equation*}
    Taking \(c_1 = a\) proves the theorem.
\end{proof}

%
%\begin{theorem}
%    Let \(\normedSpace{V}{\norm}\) be a normed space over a normed complete field \(\Field\). The following are equivalent
%    \begin{enumerate}
%        \item \(V\) is finite dimensional.
%        \item every bounded closed set in \(V\) is compact.
%        \item the closed unit ball in \(V\) is compact.
%    \end{enumerate}
%\end{theorem}
% do the proof, 3 to 1 is the hardest part
%\begin{proof}
%
%\end{proof}

\begin{example}
    The closed unit ball in the infinite dimensional vector space \(\func{C}{\clcl{0}{1},\Reals}\) with \(\norm[f] = \max \func{f}{x}\) is not compact.  Take \(\func{f_n}{x} = x^n\). Obviously \(\norm[f_n] = 1\), however \(f_n\) doesn't uniformly converge and hence \(f_n\) doesn't have a limit in \(\func{C}{\clcl{0}{1},\Reals}\) with the \(\max\) norm. Consider the following norm
    \begin{equation*}
        \norm[f]_I = \int_0^1 \abs[\func{f}{x}] \diffOperator x
    \end{equation*}
    Note that \(\norm_I\) and \(\norm_{\max} \) are not equivalent. Let \(\func{g}{x} = 0\) for all \(x \in \clcl{0}{1}\). Then
    \begin{equation*}
        \norm[f_n - g]_I = \dfrac{1}{n+1} \to 0 \quad \text{as} n \to \infty.
    \end{equation*}
\end{example}

\begin{definition}[Banach space]
    A normed vector space \(V\) that is complete is a \textbf{Banach space}. A \textbf{Hilbert Space} is a Banach space whose norm is induced by an inner product.
\end{definition}

\subsection{Linear Maps}
Let \(V\) and \(W\) be a vector spaces over \(\Field\). A map \(T: V \to W\) is \textbf{linear} if
\begin{equation*}
    \func{T}{x + \lambda y} = \func{T}{x} + \lambda \func{T}{y}
\end{equation*}
for all \(x,y \in V\) and \(\lambda \in \Field\).

\begin{definition}
    Let \(\normedSpace{V}{\norm_V}\) and \(\normedSpace{W}{\norm_W}\) be normed spaces then, a linear transformation \(T : V \to W\) is \textbf{bounded} if there exists a constant \(C > 0\) such that
    \begin{equation*}
        \norm[Tv]_W \leq C \norm[v]_V
    \end{equation*}
    for all \(v \in V\). We denote the set of all linear map from \(V \to W\) as \(\func{\CalL}{V,W}\) and the set of all bounded linear maps as \(\func{\CalB}{V,W}\). If \(T \in \func{\CalL}{V,W}\) is bijective such that \(T^{-1} \in \func{\CalL}{V,W}\), then \(T\) is called an \textbf{isomorphism} and \(V,W\) are \textbf{isomorphic}. An operator \(T \in \func{\CalL}{V,W}\) is called \textbf{isometric} if \(\norm[Tv]_W = \norm[v]_V\) for all \(v \in V\).
\end{definition}

\begin{definition}
    If \(\normedSpace{V}{\norm_V},\normedSpace{W}{\norm_W}\) are normed spaces then the \textbf{operator norm} of a linear transformation \(T : V \to W\) is
    \begin{equation*}
        \norm[T] = \sup \left\{\dfrac{\norm[Tv]_W}{\norm[v]_V} \middle| v \neq 0 \right\}
    \end{equation*}
\end{definition}

\begin{proposition}
    Let \(T : U \to V\) and \(T' : V \to W\) be two linear transformations.
    \begin{equation*}
        \norm[T' \circ T] \leq \norm[T] \norm[T']
    \end{equation*}
\end{proposition}

\begin{proof}
    for an arbitrary non-zero \(x \in U\)
    \begin{equation*}
        \norm[\func{T' \circ T}{x}]_W \leq \norm[T'] \norm[Tx]_V \leq \norm[T'] \norm[T] \norm[x]_U
    \end{equation*}
    which implies
    \begin{equation*}
        \norm[T' \circ T] \leq \norm[T] \norm[T']
    \end{equation*}
\end{proof}

\begin{theorem} \label{th:linearTransformation}
    Let \(\normedSpace{V}{\norm_V}\) and \(\normedSpace{W}{\norm_W}\) be normed spaces and \(T: V \to W\) be a linear transformation. The following are equivalent
    \begin{enumerate}
        \item \(\norm[T]\) is finite. \label{it:LinearCont1}
        \item \(T\) is bounded. \label{it:LinearCont2}
        \item \(T\) is Lipschitz continuous. \label{it:LinearCont3}
        \item \(T\) is continuous at a point. \label{it:LinearCont4}
        \item \(\sup_{\norm[v]_V = 1} \norm[Tv]_W < \infty\). \label{it:LinearCont5}
    \end{enumerate}
\end{theorem}

\begin{proof}
    %TODO: fix the references
    \cref{it:LinearCont1} \(\Rightarrow\) \cref{it:LinearCont2}: Obviously
    \begin{align*}
        \dfrac{\norm[Tv]_W}{\norm[v]_V} & \leq \norm[T]            \\
        \implies \norm[Tv]_W            & \leq \norm[T] \norm[v]_V
    \end{align*}
    note that if \(v = 0\) then \(Tv = 0\) as well and thus the last inequality holds for all \(v \in V\).

    \cref{it:LinearCont2} \(\Rightarrow\) \cref{it:LinearCont3}:
    \begin{equation*}
        \norm[Tv - Tu]_W = \norm[T(u - v)]_W \leq C \norm[u - v]_V
    \end{equation*}

    \cref{it:LinearCont3} \(\Rightarrow\) \cref{it:LinearCont4}: Trivial.

    \cref{it:LinearCont4} \(\Rightarrow\) \cref{it:LinearCont5}: Let \(T\) be continuous at \(u \in V\). Then there is  a \(\delta > 0 \) such that
    \begin{equation*}
        \norm[v-u] < \delta \implies \norm[Tv - Tu]_W = \norm[T(v-u)]_W < 1
    \end{equation*}
    Now for an arbitrary non-zero \(v\) we have
    \begin{equation*}
        \norm[\left( \dfrac{\delta v}{2\norm[v]_V} + u \right) - u]_V < \delta
    \end{equation*}
    Therefore
    \begin{align*}
         & \norm[\func{T}{\dfrac{\delta v}{2\norm[v]_V}}]_W  < 1         \\
         & \norm[\func{T}{\dfrac{v}{\norm[v]_V}}]_W  < \dfrac{2}{\delta}
    \end{align*}

    \cref{it:LinearCont5} \(\Rightarrow\) \cref{it:LinearCont1}: Let \(v \in V\) be an arbitrary vector. Then
    \begin{align*}
                 & \sup \norm[\func{T}{\dfrac{v}{\norm[v]_V}}]_W < \infty \\
        \implies & \sup \dfrac{\norm[Tv]_W}{\norm[v]_W} < \infty
    \end{align*}

\end{proof}




\begin{theorem} \label{th:finiteDimensionTransformationContinuous}
    If \(V\) is a finite dimensional normed vector space then any linear transformation \(T : V \to W\) is continuous.
\end{theorem}

\begin{proof}
    Since \(V\) is finite dimensional, according to \Cref{th:normsEquivalent}, any two norms are equivalent. Hence, take \(\norm_2\) to be Euclidean norm over a basis \(\{e_1, \dots , e_n\}\). Let \(x\) be such that \(\norm[x]_2 < \delta\) for some \(\delta > 0\). Therefore, \(\abs[\xi_i] < \delta^2\)
    \begin{equation*}
        \norm[Tx]_W = \norm[\sum \xi_i \func{T}{e_i}]_W \leq \sum \abs[\xi_i] \norm[\func{T}{e_i}]_W \leq \delta^2 K
    \end{equation*}
    where \(K = \max \norm[\func{T}{e_i}]_W \). By letting \(\delta = \sqrt{\frac{\epsilon}{K}}\) we proved continuity at \(0\) and hence the continuity by \Cref{th:linearTransformation}.
\end{proof}

\begin{corollary}
    Any finite dimensional normed vector space \(V\) over a normed complete field \(\Field\) is a Banach space.
\end{corollary}

\begin{proof}
    Let \(\{e_1, \dots , e_n\}\) be a basis for \(V\) and \(\phi : V \to \Field^n\) be the representation map for the basis. Since \(\phi\) is a linear map and a bijection then \(\phi\) is homeomorphism. Consider a Cauchy sequence \(\set{v_k} \in V\) and let \(x_k = \func{\phi}{v_k}\) then by continuity of \(\phi\) and \(\phi^{-1}\) we have
    \begin{equation*}
        \abs[x_i - x_j] = \abs[\func{\phi}{v_i} - \func{\phi}{v_j}] \leq \norm[\phi] \norm[v_i - v_j] \leq \norm[\phi] \norm[\func{\phi^{-1}}{x_i} - \func{\phi^{-1}}{x_j}] \leq \norm[\phi] \norm[\phi^{-1}] \abs[x_i - x_j]
    \end{equation*}
    hence \(\set{x_k}\) are Cauchy in \(\Field^n\) which by completeness of \(\Field\) implies that they are convergent, \(x_k \to x\). Let \(v = \func{\phi^{-1}}{x}\) then by the right side of the inequality \(v_k \to v\).
\end{proof}

\begin{remark}
    As seen in the last proof, for a bijective linear transformation \(T\)
    \begin{equation*}
        1 \leq \norm[T] \norm[T^{-1}]
    \end{equation*}
\end{remark}

\begin{theorem}
    For two normed vector spaces \(V,W\), \(\normedSpace{\func{\CalB}{V,W}}{\norm[T]}\) is a normed vector space. Moreover, it is a Banach space when \(W\) is a Banach space.
\end{theorem}


\begin{proof}
    Clearly \(\func{\CalB}{V,W}\) is a vector space. For its norm \(\norm[T]\) we have
    \begin{enumerate}
        \item \(\norm[T] \geq 0\) by definition.
        \item if \(\alpha \in \Field_W\) then
              \begin{equation*}
                  \norm[\alpha T] = \sup \left\{ \dfrac{\norm[(\alpha T)v]_W}{\norm[v]_V} \middle| v \neq 0 \right\} = \abs[\alpha] \sup \left\{ \dfrac{\norm[Tv]_W}{\norm[v]_V} \middle| v \neq 0 \right\} = \abs[\alpha] \norm[T]
              \end{equation*}
        \item for the triangle inequality
              \begin{align*}
                  \norm[T_1 + T_2] & = \sup \left\{ \dfrac{\norm[(T_1 + T_2)v]_W}{\norm[v]_V} \right\}                                                     \\
                                   & \leq \sup \left\{ \dfrac{\norm[T_1v]_W + \norm[T_2v]_W}{\norm[v]_V} \right\}                                          \\
                                   & = \sup \left\{ \dfrac{\norm[T_1v]_W}{\norm[v]_V} \right\} + \sup \left\{ \dfrac{\norm[ T_2 v]_W}{\norm[v]_V} \right\} \\
                                   & = \norm[T_1] + \norm[T_2]
              \end{align*}
    \end{enumerate}
    Suppose \(W\) is a Banach space and \(\{T_i\} \in \func{\CalB}{V,W}\) is a Cauchy sequence. Then for all \(v \in V\)
    \begin{equation*}
        \forall \epsilon \, \exists N \; \suchThat \; m,n > N \implies \norm[T_m v - T_n v]_W \leq \norm[T_m - T_n]\norm[v]_V < \epsilon
    \end{equation*}
    \(\{T_i v\}\) is a Cauchy sequence. Since \(W\) is complete then \(T_i v \to Tv\) for some function \(T\). We claim that \(T\) is a bounded linear map and is the limit of \(T_i \to T\).
    \begin{align*}
        \func{T}{v + cu} & = \lim_{i \to \infty} \func{T_i}{v + cu} = \lim_{i \to \infty} T_i v + c T_i u \\
                         & = Tv + c Tu
    \end{align*}
    Note that  \( \abs[{\norm[T_m] - \norm[T_n]}] \leq \norm[T_m - T_n]\) and hence \(\norm[T_i]\) is a Cauchy in sequence in \(\Reals\) that has a limit \(t\). There exists a \(N\) such that \(\abs[{\norm[T_n] - t}] < 1\) for all \(n \geq N\).
    \begin{equation*}
        \dfrac{\norm[Tv]_W}{\norm[v]_V} = \lim_{i \to \infty}  \dfrac{\norm[T_i v]_W}{\norm[v]_V} < t + 1
    \end{equation*}
    hence \(T\) is bounded and \(T \in \func{\CalB}{V,W}\). Finally, we show that \(T_i \to T\). For an arbitrary \(v \neq 0\) and \(\epsilon > 0\) there exist \(N\) such that
    \begin{align*}
        n \geq N \implies \norm[T_i v - Tv]_W < \epsilon \norm[v]_V
    \end{align*}
    which means that
    \begin{equation*}
        \norm[T_i - T] = \sup \dfrac{\norm[T_i v - Tv]_W}{\norm[v]_V} < \epsilon
    \end{equation*}
    Therefore \(T_i \to T\) as desired.
\end{proof}

\begin{theorem}
    Let \(\normedSpace{V}{\norm}\) be a normed space. Then any linear transformation \(T: \Reals^n \to V\) is continuous. Furthermore, if \(T\) is a bijection, it is a homeomorphism.
\end{theorem}

%TODO: a better proof
\begin{proof}
    Since \(\Reals^n\) is finite then by \Cref{th:finiteDimensionTransformationContinuous}, \(T\) is continuous. Assuming \(T\) is bijective, we must show that its inverse \(T^{-1}\) is continuous as well. Since \(T\) is a bijection then \(T\) is a linear isomorphism and \(\dim V = \dim \Reals^n = n\) hence \(T^{-1}\) is a continuous map.
\end{proof}

\begin{definition}[General linear group]
    The \textbf{general linear group} of a vector space, written \(\func{\GL}{V}\) is the set of all bijective linear transformation.
\end{definition}

\begin{proposition}
    If \(V\) is a finite (also works for infinite) vector space then \(\func{\GL}{V}\) is open in \(\func{\CalL}{V,V}\), in fact, if \(f \in \func{\GL}{V}\) then the open ball centered at \(f\) with radius \(\norm[f^{-1}]^{-1}\) remains in \(\func{\GL}{V}\). Furthermore, the inverse operator \(i : \func{\GL}{V} \to \func{\GL}{V}\), \(\func{i}{T} = T^{-1}\) is continuous.
\end{proposition}

\begin{proof}
    First assume \(f = \DSOne_V\) then we prove that any linear \(g\) that \(\norm[\DSOne_V - g] < 1\) is invertible which then implies bijectivity (true for linear maps). Let \(\norm[v] = 1\) then
    \begin{equation*}
        \abs[\norm[v] - \norm[gv]] \leq \norm[v - gv] \leq \norm[\DSOne_V - g] \norm[v] < 1
    \end{equation*}
    Therefore
    \begin{equation*}
        0 < \norm[gv] < 2
    \end{equation*}
    which means \(\ker g = \set{0}\) and since \(V\) is finite then then \(g\) is invertible. For a general \(f\), we have that
    \begin{equation*}
        \norm[1 - f^{-1} \circ g] \leq \norm[f^{-1}]\norm[f -g] < 1
    \end{equation*}
    therefore \(f^{-1} \circ g\) is invertible and as a consequence \(g = f \circ f^{-1} \circ g\) is invertible. To prove inverse operator is continuous, fix \(\epsilon > 0\) then for a \(\delta > 0\) if \(\norm[T-S] < \delta\) then
    \begin{align*}
        \norm[\DSOne_V- T^{-1} \circ S]= \norm[T^{-1} \circ T  - T^{-1} \circ S]           & \leq \norm[T^{-1}] \norm[T-S] < \delta \norm[T^{-1}] \\
        \implies  \norm[T^{-1} - S^{-1}] \leq \norm[T^{-1}\circ S - \DSOne_V]\norm[S^{-1}] & = \delta \norm[T^{-1}] \norm[S^{-1}]
    \end{align*}
    note that by letting \(\delta = \norm[T^{-1}]^{-1}/2\) then
    \begin{equation*}
        \norm[S] > -\dfrac{\norm[T^{-1}]^{-1}}{2} + \norm[T] > \dfrac{\norm[T^{-1}]^{-1}}{2}
    \end{equation*}
    also if for any invertible linear map \(R\)
    \begin{equation*}
        \norm[R] > a \implies \norm[Rx] > a\norm[x] \implies \dfrac{\norm[y]}{a} = \dfrac{\norm[R\circ \func{R^{-1}}{y}]}{a} > \norm[R^{-1}y]
    \end{equation*}
    which means that \(\norm[S^{-1}] < 2 \norm[T^{-1}]\), hence by letting
    \begin{equation*}
        \delta = \min \set{\dfrac{\epsilon \norm[T^{-1}]^2}{2} , \dfrac{\norm[T^{-1}]^{-1}}{2}}
    \end{equation*}
    we proved the continuity.
\end{proof}

\begin{theorem}
    \(T : \Reals^n \to \Reals^n\) linear transformation is invertible if and only if there exists a \(c\) such that:
    \begin{equation*}
        c \norm[x] \leq \norm[Tx]
    \end{equation*}
\end{theorem}

\begin{proof}
    If \(T\) is invertible then \(T^{-1} : \Reals^n \to \Reals^n \) is bounded and thus
    \begin{equation*}
        \norm[T^{-1}x] \leq c \norm[x]
    \end{equation*}
    and since \(T\) is bijective then there exists \(y\) such that \(x = Ty\) which implies
    \begin{equation*}
        \norm[y] \leq c\norm[Ty]
    \end{equation*}
    If there exists such \(c\) then \(\norm[Tx] > 0\) for all non-zero \(x\) and hence \(\ker T  = 0\) which implies that \(T\) is a bijection and hence \(T\) is invertible.
\end{proof}

\begin{definition}
    Let \(V_1, V_2 ,\dots , V_n\) be  normed vector spaces. Then the function \(\phi : V_1 \times \dots \times V_n \to W\) is \textbf{\(n\)-linear} if by fixing any \(n-1\) component, \(\phi\) is linear relative to the remaining components.
\end{definition}

\begin{proposition}
    If \(V_1, V_2, \dots , V_n\) are  normed vector spaces and \(\ \phi : V_1 \times \dots \times V_n \to W \) is a \(n\)-linear then the followings are equivalent
    \begin{enumerate}
        \item \(\phi\) is continuous. \label{it:continuityOfnLinear}
        \item \(\phi\) is continuous at a point. \label{it:continuityOfnLinearataPoint}
        \item \(\phi\) is bounded, that is there exists a constant \(C > 0\) such that \label{it:boundednessOfnLinear}
              \begin{equation*}
                  \norm[\func{\phi}{v_1, \dots ,v_n}]_W \leq C \norm[v_1]_{V_1} \dots \norm[v_n]_{V_n}
              \end{equation*}
    \end{enumerate}
\end{proposition}


%TODO: do the proof dumbass
\begin{proof}
    \Cref{it:continuityOfnLinear} \(\implies\) \Cref{it:continuityOfnLinearataPoint}: Trivial.

    \Cref{it:continuityOfnLinearataPoint} \(\implies\) \Cref{it:boundednessOfnLinear}
\end{proof}

{\Large\textbf{Exercises}}
\begin{enumerate}
    \item Show that for a linear transformation \(T\), \(\norm[T] = \sup_{\norm[v]_V \leq 1} \norm[Tv]_W\).
\end{enumerate}
\newpage

\section{Derivative}
Let \(V,W\) be finite dimensional vector spaces and \(f: U \subset V \to W\) where \(U\) is open. Then \(f\) is differentiable at \(x_0\) when a linear transformation \(T : V \to W\) such that
\begin{equation*}
    \lim_{\norm[h] \to 0} \dfrac{\norm[\func{f}{x_0 + h} - \func{f}{x_0} - \func{T}{h}]}{\norm[h]}= 0
\end{equation*}
Or equivalently there exists a sublinear function \(\func{R}{h}\) such that
\begin{equation*}
    \func{f}{x_0 + h} - \func{f}{x_0} - Th = \func{R}{h} \qquad \frac{\func{R}{h}}{\norm[h]} \to 0
\end{equation*}
\(T\) if it exists is unique, represented by \(\func{f'}{x_0}\), \(\DiffOperator f\), or \(\func{\diffOperator f}{x}\) and called the \textbf{total derivative} or \textbf{Fr\'{e}chet derivative}.

\begin{example}
    Every linear function \(f : V \to W\) with \(\func{f}{v} = Tv + b\) where \(b \in W\) is differentiable and \(\func{\DiffOperator f}{v} = T\). Since
    \begin{equation*}
        \norm[h]_V < \delta \implies \norm[\func{f}{v + h} - \func{f}{v} - \operatorFunc{\DiffOperator \func{f}{v}}{h}]_W = \norm[T(v+h) - Tv - Th]_W = 0 < \epsilon \norm[h]_V
    \end{equation*}
    Hence, the derivative of any linear function is constant.
    Consider \(S : V \times V \to V\) with \(\func{S}{v,u} = v + u\). \(S\) is differentiable because \(S\) is linear (why?). We claim that \(\DiffOperator S = S\) as
    \begin{equation*}
        \norm[\func{S}{(v + h),(u + k)} - \func{S}{v,u} - \func{S}{h,k}] = 0
    \end{equation*}
\end{example}

\begin{example}
    Let \(\mu : \Reals \times V \to V\) with \(\func{\mu}{r,x} = rx\). Then \(\mu\) is differentiable and \(\operatorFunc{\func{\DiffOperator \mu}{r,x}}{t,h} = rh + tx\) as
    \begin{align*}
        \norm[\func{\mu}{(r + t),(x + h)} - \func{\mu}{r,x} - \operatorFunc{\func{\DiffOperator \mu}{r,x}}{t,h}] & = \norm[rx + rh + tx + th - rx - rh - tx]     \\
                                                                                                                 & = \abs[t] \norm[h] \leq \epsilon \norm[(t,h)]
    \end{align*}
    by letting \(\norm[(t,h)] = \sqrt{t^2 + \norm[h]^2}\) and \(\delta = \epsilon\).
\end{example}

\begin{proposition}
    Differentiability of \(f\) at \(x\) implies continuity at \(x\).
\end{proposition}

\begin{proof}
    \begin{equation*}
        \norm[\func{f}{x + h} - \func{f}{x}] = \norm[\operatorFunc{\DiffOperator \func{f}{x}}{h} + \func{R}{h}] \leq \norm[\DiffOperator \func{f}{x}]\norm[v] + \norm[\func{R}{v}] \to 0
    \end{equation*}
    as \(v \to 0\).
\end{proof}

\begin{proposition} \label{eq:partialDerivative}
    Assume \(f: U \subset V \to W\) is differentiable at \(x_0\) and let \(u \in V\) be a non-zero vector then
    \begin{equation*}
        \func{f'}{x_0} (u) = \lim_{t \to 0} \dfrac{\func{f}{x_0 + tu} - \func{f}{x_0}}{t}
    \end{equation*}
\end{proposition}

\begin{proof}
    Let \(h = tu\) then
    \begin{align*}
        \func{R}{tu}                     & = \func{f}{x_0 + tu} - \func{f}{x_0} - \func{T}{tu}            \\
                                         & = \func{f}{x_0 + tu} - \func{f}{x_0} - t\func{T}{u}            \\
        \implies \dfrac{\func{R}{tu}}{t} & = \dfrac{ \func{f}{x_0 + tu} - \func{f}{x_0}}{t} - \func{T}{u} \\
        \implies \lim_{t \to 0}          & \dfrac{ \func{f}{x_0 + tu} - \func{f}{x_0}}{t} = \func{T}{u}
    \end{align*}
\end{proof}

\begin{definition}[Directional derivative]
    If we let \(\norm[u] = 1\) then the limit in \Cref{eq:partialDerivative} becomes the \textbf{directional derivative} of \(f\) in the direction of \(u\) and is denoted by \(\DiffOperator_u f\).
\end{definition}

\begin{remark}
    The existence of all directional derivatives of \(f\) doesnt imply its differentiability or even its continuity.
\end{remark}

\begin{remark}
    If \(\DiffOperator f: U \to \func{\CalL}{V,W}\) is continuous then each \(\PDiff{f_i}{x_j}\) is continuous. Since
    \begin{equation*}
        \DiffOperator \func{f}{x} = \begin{bmatrix}
            \func{\PDiff{f_1}{x_1}}{x} & \dots  & \func{\PDiff{f_1}{x_n}}{x} \\
            \vdots                     & \ddots &                            \\
            \func{\PDiff{f_m}{x_1}}{x} & \dots  & \func{\PDiff{f_m}{x_n}}{x}
        \end{bmatrix}
    \end{equation*}
    and the reverse is true as well.
\end{remark}

\begin{theorem}
    \(f : V \to W\) has all of its partial derivative in a neighbourhood of \(u \in U\) and they're continuous at \(u\) then \(f\) is differentiable at \(u\). Especially, if \(\PDiff{f_i}{x_j}\) exist and are continuous at every point of \(u\) then \(f \in \CalC^1\).
\end{theorem}

\begin{proof}
    We prove that each \(f_i\) is differentiable. Let \(\set{e_1, \dots , e_n}\) be a basis for \(V\) and take \(\norm[x] = \sum \abs[\xi_j]\). Consider a convex neighbourhood \(E\) of \(a\). Then, for a given \(\epsilon > 0\) we will show there exists a \(\delta > 0\) such that
    \begin{equation*}
        \norm[h] < \delta \implies \norm[\func{f_i}{a + h} - \func{f_i}{a} - \sum_{j = 1}^n \operatorFunc{\DiffOperator_{e_j} \func{f_i}{a}}{h_j}] \leq \epsilon \norm[h]
    \end{equation*}
    Cosider the point sequence \(a^k =\sum_{j < k} a_j e_j + \sum_{j \geq k} (a_j + h_j)e_j \) where \(a^1 = a + h\) and \(a^{n + 1} = a\) then
    \begin{equation*}
        \norm[\func{f_i}{a + h} - \func{f_i}{a} - \sum_{j = 1}^n \operatorFunc{\DiffOperator_{e_j} \func{f_i}{a}}{h_j}]  \leq \sum_{k = 1}^{n} \norm[\func{f_i}{a^k} - \func{f_i}{a^{k+1}} - \operatorFunc{\DiffOperator_{e_k} \func{f_i}{a}}{h_k}]
    \end{equation*}
    hence we are done if
    \begin{equation*}
        \norm[\func{f_i}{a^k} - \func{f_i}{a^{k+1}} - \operatorFunc{\DiffOperator_{e_k} \func{f_i}{a}}{h_k}] \leq \epsilon \abs[h_k]
    \end{equation*}
    for \(k = 1\) it is equivalent to the existence first partial derivative. and for the rest we use the continuity.
\end{proof}

\begin{proposition}
    Let \(f,g : V \to W\) be differentiable at \(x\) and \(h : W \to U\) be differentiable at \(y = \func{f}{x}\). Furthermore, let \(c\) be an scalar then
    \begin{enumerate}
        \item \(\func{\DiffOperator \,}{ f + cg} = \DiffOperator f + c \DiffOperator g\).
        \item  \(h \circ f\) is differentiable at \(x\) and
              \begin{equation*}
                  \func{\DiffOperator \,}{ h \circ f} =  \left( \func{\DiffOperator \,}{h}\circ f \right) \circ \func{\DiffOperator \, }{ f}
              \end{equation*}
        \item For a bilinear function \(\beta\)
              \begin{equation*}
                  \operatorFunc{\DiffOperator \func{\beta}{f,g}}{v} = \func{\beta}{\operatorFunc{\DiffOperator f}{v},g} + \func{\beta}{f, \operatorFunc{\DiffOperator g}{v}}
              \end{equation*}
    \end{enumerate}
\end{proposition}

\begin{proposition}
    \(f : U \subset V \to W_1 \times \dots \times W_n\) is differentiable at \(x_0\) if and only if all its component is differentiable at \(x_0\). Furthermore, \(\DiffOperator f = (\DiffOperator f_1, \dots , \DiffOperator f_n)\).
\end{proposition}

\begin{theorem}[Leibnitz rule]
    Let \(V_1, V_2, \dots , V_n\) be finite dimensional vector spaces and \(f: V_1 \times \dots \times V_n \to W\) is a \(n\)-linear function. \(f\) is differentiable at \(a = (a_1, \dots , a_n)\) and
    \begin{equation*}
        \operatorFunc{\func{\DiffOperator f}{a}}{h_1, \dots h_n} = \func{f}{h_1, a_2, \dots, a_n} + \func{f}{a_1, h_2, \dots, a_n} + \dots + \func{f}{a_1, a_2, \dots, h_n}
    \end{equation*}
\end{theorem}

%TODO: [[]]
\begin{proof}

\end{proof}

\begin{example}
    Let \(Z: \Reals^3 \times \Reals^3 \to \Reals\) with \(\func{Z}{u,v}= u \times v\) be a bilinear function, \(f,g: \Reals \to \Reals^3\) and \(\func{h}{t} = \func{f}{t} \times \func{g}{t}\). \(h = Z \circ \phi\) where \(\func{\phi}{t} = (\func{f}{t},\func{g}{t})\). Then we have:
    \begin{align*}
        \DiffOperator \func{h}{t} & = \operatorFunc{\DiffOperator Z}{\func{\phi}{t}}\circ \DiffOperator \func{\phi}{t}                             \\
                                  & =  \operatorFunc{\DiffOperator Z}{\func{\phi}{t}} \circ (\DiffOperator \func{f}{t}, \DiffOperator \func{g}{t}) \\
                                  & = \func{Z}{\DiffOperator \func{f}{t}, \func{g}{t}} + \func{Z}{ \func{f}{t}, \DiffOperator \func{g}{t}}         \\
                                  & = \DiffOperator \func{f}{t} \times \func{g}{t} + \func{f}{t} \times \DiffOperator \func{g}{t}
    \end{align*}
\end{example}

\begin{example}
    Consider \(A = [\func{f_{ij}}{x_1, \dots , x_n}]\) where each \(f_{ij}\) is differentiable. Then
    \begin{equation*}
        \DiffOperator \func{\det}{A}
    \end{equation*}
    can be calculated using the Leibnitz rule, since determinant is \(n\)-linear function.
\end{example}

\subsection{Mean value theorem}
in general doesnt work \(f(t) = (t^2,t^3)\) however it works on a convex domain to reals.

\begin{theorem}
    Let \(V,W\) be normed finite dimensional vector spaces and \(f: U \to W\) is differentiable and \(A,B \in U\) are such that the line connecting in completely contained in \(U\) and for each \(p\) on that line
    \begin{equation*}
        \norm[\DiffOperator \func{f}{p}] \leq M
    \end{equation*}
    then
    \begin{equation*}
        \norm[\func{f}{B} - \func{f}{A}]_W \leq M \norm[B - A]_V
    \end{equation*}
\end{theorem}
First consider the following lemma:
Assume the following lemma
\begin{lemma} \label{lm:MeanValueTheoremLemma}
    If \(\phi: \clcl{0}{1} \to W\) is continuous, differentiable on \(\opop{0}{1}\) and \(\norm[\func{\phi'}{t}] \leq M\) for all \(t \in \opop{0}{1}\) then
    \begin{equation*}
        \norm[\func{\phi}{1} - \func{\phi}{0}]_W \leq M
    \end{equation*}
\end{lemma}

\begin{prooflemma}
    We provide three proofs for the lemma
    \begin{enumerate}
        \item Assuming the norm on \(W\) is induced by an inner product. Then, let \( e = \frac{\func{\phi}{1} - \func{\phi}{0}}{\norm[\func{\phi}{1} - \func{\phi}{0}]}\) be a unit vecor in \(W\) then \(\psi : \clcl{0}{1} \to \Reals\), \(\func{\psi}{t} = e \cdot \func{\phi}{t}\) is continuous and differentiable on \(\opop{0}{1}\). By the mean the value theorem
              \begin{align*}
                  \abs[\func{\psi}{1} - \func{\psi}{0}]                        & = \abs[\func{\psi'}{t_0}]       \\
                  \abs[e \cdot \left( \func{\phi}{1} - \func{\phi}{0} \right)] & = \abs[e \cdot \func{\phi'}{t}] \\
                  \norm[\func{\phi}{1} - \func{\phi}{0}]                       & \leq M
              \end{align*}
        \item Using the Hahn-Banach theorem, that is for a finite dimensional vector space \(V\) and \(e \in V\) with \(\norm[v] = 1\) then there exists a linear function \(\theta : V \to \Reals\) such that \(\norm[\theta] = 1\) and \(\func{\theta}{e} = 1\). Now let \(\func{\psi} = \func{\theta}{\func{\phi}{t}}\) then
              \begin{align*}
                  \abs[\func{\psi}{1} - \func{\psi}{0}]                & = \abs[\func{\psi'}{t_0}]                                                             \\
                  \abs[\func{\theta}{\func{\phi}{1} - \func{\phi}{0}}] & = \operatorFunc{\DiffOperator \func{\theta}{\func{\phi}{t_0}}}{\func{\phi'}{t_0}}     \\
                  \norm[\func{\phi}{1} - \func{\phi}{0}]               & = \func{\theta}{\func{\phi'}{t_0}} \leq \norm[\theta] \norm[\func{\phi'}{t_0}] \leq M
              \end{align*}
        \item From Hoimander. For any \(\epsilon\) consider the set \(T_\epsilon\).
              \begin{equation*}
                  T_\epsilon = \set[t \in \clcl{0}{1}]{\forall s, \; 0 \leq s \leq t, \; \norm[\func{\phi}{s} - \func{\phi}{0}] \leq(M + \epsilon)s + \epsilon}
              \end{equation*}
              first note that \(T_\epsilon = \clcl{0}{c}\) and \(c > 0\) because for \(s = 0\) the inequality is strict and both sides are continuous with respect to \(s\). We claim that \(c = 1\) because otherwise \(c < 1\) we have, by differentiability of \(\phi\), there exists a \(\delta < 1 - c\) such that if
              \begin{align*}
                  \norm[h] < \delta \implies \norm[\func{\phi}{c + h} - \func{\phi}{c} - \operatorFunc{\DiffOperator \func{\phi}{c}}{h}] & \leq \epsilon \norm[h]                                        \\
                  \implies \norm[\func{\phi}{c + h} - \func{\phi}{c}]                                                                    & \leq\norm[h] \left( \epsilon + \norm[\func{\phi'}{c}] \right) \\
                                                                                                                                         & \leq \norm[h] (\epsilon + M)
                  \intertext{also since \(c \in T_\epsilon\)}
                  \norm[\func{\phi}{c} - \func{\phi}{0}]                                                                                 & < (M + \epsilon)c + \epsilon                                  \\
                  \implies \norm[\func{\phi}{c + h} - \func{\phi}{0}]                                                                    & < (M + \epsilon)(c + h) + \epsilon \qquad 0 < h < \delta
              \end{align*}
              hence \(c + h \in T_\epsilon\) which is a contradiction and thus \(c = 1\).
    \end{enumerate}
\end{prooflemma}

\begin{proof}
    Let \(\sigma : \clcl{0}{1} \to U\) is the parameterization of the line connecting \(A\) to \(B\), \(\func{\sigma}{t} = (1 - t) A + tB\). Let \(\phi = f \circ \sigma\) then clearly \(\phi\) is continuous on \(\clcl{0}{1}\) and differentiable on \(\opop{0}{1}\) and we have
    \begin{align*}
        \func{\phi'}{t} & = \operatorFunc{\DiffOperator \func{f}{\func{\sigma}{t}}}{\func{\sigma'}{t}}
                        & = \operatorFunc{\DiffOperator \func{f}{\func{\sigma}{t}}}{B - A}
        \implies \norm[\func{\phi'}{t}] \leq \norm[\DiffOperator \func{f}{\func{\sigma}{t}}] \norm[B-A]_V \leq M \norm[B-A]_V
    \end{align*}
    therefore by the \Cref{lm:MeanValueTheoremLemma}
    \begin{equation*}
        \norm[func{f}{B} - \func{f}{A}]_W = \norm[\func{\phi}{1} - \func{\phi}{0}]_W \leq M \norm[B-A]_V
    \end{equation*}
\end{proof}

\begin{corollary}
    Let \(U \subset V\) is connected and open and \(f: U \to W\) is differentiable and \(\DiffOperator \func{f}{u} = 0\) for all \(u \in U\) then \(f\) is constant.
\end{corollary}

\begin{proof}
    closedness easy, openness from the MVT.
\end{proof}

\begin{corollary}
    Let \(V_1, V_2, W\) be finite dimensional normed vector space and \(U \subset V_1 \times V_2\) is open such that for every \(y \in V_2\) the intersection \((V_1 \times \set{y}) \cap U\) is connected. Assumne \(f : U \to W\) is differentiable and \(\DiffOperator_{V_1} \func{f}{x,y} = 0\) for all \((x,y) \in U\) then for any \((x_1,y), (x_2,y) \in U\),\(\func{f}{x_1,y} = \func{f}{x_2,y}\).
\end{corollary}

